Your questions about managing backups for a 100TB Citus cluster with a replication factor of 2, particularly in the context of disaster recovery (DR), auditing, and shard rebalancing, are critical for ensuring data integrity and operational continuity. Since you’re running a Citus cluster with PostgreSQL (likely upgrading from 15 to 17 based on prior conversations, using a non-standard data directory `/var/tellme/pgsql/data`, and an RPM-based system), I’ll provide a detailed, tailored response addressing:
1. Whether you should still run backups with a replication factor of 2.
2. Whether backups are needed on each worker.
3. How to use backups for auditing purposes if a worker fails.
4. Whether Citus shard rebalancing automatically restores data to a recovered or new worker.

I’ll also incorporate Citus-specific concepts and practical considerations for a 100TB cluster, ensuring the response is concise yet comprehensive.

---

### 1. Should You Still Run Backups with Replication Factor 2?
**Yes, you should still run backups**, even with a Citus replication factor of 2 (`citus.shard_replication_factor = 2`). While shard replication provides high availability (HA) and protects against single worker node failures, it is not a substitute for backups. Here’s why:

#### Why Backups Are Necessary
- **Replication Factor 2 Limitations**:
  - A replication factor of 2 means each shard has two copies, stored on different worker nodes. This protects against the failure of a single worker, as queries can be rerouted to the surviving replica.
  - However, replication does not protect against:
    - **Multiple simultaneous failures**: If two or more workers fail (e.g., due to a data center issue, hardware failure, or misconfiguration), you could lose both replicas of some shards, leading to data loss.
    - **Corruption**: Data corruption (e.g., from software bugs, hardware issues, or human error) can propagate to shard replicas, as replication copies all changes, including corrupted data.
    - **Coordinator failure**: The coordinator node stores critical metadata (e.g., `pg_dist_partition`, `pg_dist_shard`). If the coordinator fails without a backup, you could lose the ability to reconstruct the cluster’s distributed structure.
    - **Accidental deletion**: If a table or database is dropped (e.g., human error), replication does not provide a way to recover the deleted data, as the drop is replicated to all replicas.
    - **Data center outages**: Shard replication within a single cluster (same data center) does not protect against a full data center failure.
- **Backups for Disaster Recovery (DR)**:
  - Backups provide a **point-in-time snapshot** of your data, allowing recovery from catastrophic failures (e.g., multiple node failures, corruption, or data center loss).
  - They enable **point-in-time recovery (PITR)** when combined with Write-Ahead Log (WAL) archiving, allowing you to restore to a specific moment before an incident.
- **Auditing and Compliance**:
  - For auditing purposes (as you mentioned), backups are essential to retain historical data snapshots for regulatory compliance, legal requirements, or internal audits. Replication does not serve this purpose, as it only maintains the current state of the data.
- **Cluster Reconstruction**:
  - In a worst-case scenario (e.g., loss of the entire cluster), backups of the coordinator and workers allow you to rebuild the Citus cluster, including metadata and shard data.

#### Recommendation
- **Run regular backups** for both the coordinator and worker nodes, regardless of the replication factor.
- For a 100TB cluster, backups are critical due to the scale and potential cost of data loss. Use a combination of **logical backups** (e.g., `pg_dump`) for metadata and smaller datasets and **physical backups** (e.g., `pg_basebackup` or filesystem snapshots) for the entire cluster.

---

### 2. Should You Back Up Each Worker?
**Yes, you should back up each worker node**, in addition to the coordinator node, to ensure comprehensive DR and auditing capabilities. However, the backup strategy for workers can be optimized due to the replication factor. Here’s a detailed breakdown:

#### Why Back Up Each Worker?
- **Shard Distribution**:
  - In a Citus cluster, each worker stores a subset of shards for distributed tables. With `replication_factor = 2`, every shard has a replica on another worker, but no single worker has all shards.
  - Backing up each worker ensures you capture all shards, as the data is distributed across the cluster.
- **Coordinator Metadata**:
  - The coordinator node stores metadata (e.g., `pg_dist_shard`, `pg_dist_placement`) that maps shards to workers. Backing up the coordinator is critical to reconstruct the cluster’s structure.
  - However, the coordinator does not store the actual shard data, so worker backups are necessary to recover the data itself.
- **Redundancy and Recovery**:
  - While replication reduces the urgency of backing up every worker (since shards are replicated), backing up all workers ensures you can recover from scenarios where multiple workers fail or data corruption affects replicas.
  - For a 100TB cluster, the sheer volume of data makes it impractical to rely solely on rebalancing or rebuilding from surviving replicas, as this could be time-consuming and risky.

#### Optimizing Worker Backups
- **Selective Backups**:
  - With `replication_factor = 2`, you could theoretically back up only a subset of workers that collectively cover all shards (e.g., half the workers, assuming perfect shard distribution). The coordinator’s metadata can help identify which workers have unique shard replicas.
  - However, this approach is risky because:
    - It assumes perfect shard placement and no corruption in the surviving replicas.
    - Reconstructing the cluster from a subset of workers is complex and error-prone.
  - **Recommendation**: Back up all workers to ensure redundancy and simplify recovery.
- **Incremental Backups**:
  - For a 100TB cluster, full backups of all workers are resource-intensive. Use **incremental physical backups** (e.g., with tools like `pgBackRest` or `Barman`) to reduce storage and time requirements.
  - Combine with **WAL archiving** for PITR, allowing you to restore workers to a consistent state.
- **Logical Backups for Metadata**:
  - Back up the coordinator’s metadata using `pg_dump` or `COPY` commands to export `pg_dist_*` tables:
    ```sql
    COPY pg_dist_partition TO '/var/tmp/pg_dist_partition.data';
    COPY pg_dist_shard TO '/var/tmp/pg_dist_shard.data';
    COPY pg_dist_placement TO '/var/tmp/pg_dist_placement.data';
    ```
  - This ensures you can reconstruct the cluster’s shard mappings even if the coordinator’s data directory is lost.

#### Backup Strategy for Workers
- **Physical Backups**:
  - Use `pg_basebackup` or a backup tool like `pgBackRest` to back up each worker’s data directory (`/var/tellme/pgsql/data`).
  - Example with `pgBackRest`:
    ```bash
    pgbackrest --stanza=worker1 backup
    ```
  - Configure WAL archiving to enable PITR:
    ```conf
    # postgresql.conf
    wal_level = replica
    archive_mode = on
    archive_command = 'pgbackrest --stanza=worker1 archive-push %p'
    ```
- **Frequency**:
  - Perform full backups weekly or monthly, with daily incremental backups and continuous WAL archiving.
  - For a 100TB cluster, stagger backups across workers to avoid performance impacts (e.g., back up one worker per day).
- **Storage**:
  - Store backups off-site (e.g., cloud storage like AWS S3 or a separate data center) to protect against data center failures.
  - Compress backups to manage storage costs (e.g., `pgBackRest` supports compression).

#### Coordinator Backup
- **Critical**: The coordinator’s metadata is essential for cluster operation. Back up its data directory (`/var/tellme/pgsql/data`) and export `pg_dist_*` tables regularly.
- Use `pg_basebackup` or `pgBackRest` for physical backups and `pg_dump` for logical backups of metadata schemas.

---

### 3. How to Use Backups for Auditing Purposes if a Worker Fails?
For **auditing purposes**, backups serve as historical snapshots to review past data states (e.g., for compliance, legal requirements, or investigating data changes). If a worker fails, you can use backups to access historical data for auditing, even if Citus shard replication or rebalancing restores the cluster’s operational state. Here’s how to use backups in this scenario:

#### Scenario: Worker Failure
- Suppose a worker node fails, and Citus reroutes queries to surviving shard replicas (due to `replication_factor = 2`). You may need to audit data from the failed worker (e.g., to check data as of a specific date for compliance).

#### Using Backups for Auditing
1. **Identify the Relevant Backup**:
   - Use the backup of the failed worker’s data directory (e.g., `/var/tellme/pgsql/data_backup` from `pgBackRest` or `pg_basebackup`).
   - If using PITR, select a backup and WAL files to restore to the desired point in time (e.g., the date of interest for auditing).

2. **Restore the Backup**:
   - Restore the worker’s backup to a separate, non-production PostgreSQL instance (to avoid affecting the live cluster):
     ```bash
     # Example with pgBackRest
     pgbackrest --stanza=worker1 --target-time="2025-05-01 12:00:00" restore --db-path=/var/tellme/pgsql/audit_data
     ```
   - Start a PostgreSQL 17 instance on the restored data directory:
     ```bash
     /usr/pgsql-17/bin/pg_ctl -D /var/tellme/pgsql/audit_data -l logfile start
     ```
   - Note: You don’t need Citus for this instance, as you’re only querying the worker’s local data (shards).

3. **Query the Restored Data**:
   - Connect to the restored instance and query the shard tables directly. For example, if auditing a distributed table `my_table`, query its shard tables (e.g., `my_table_12345`).
   - Use the coordinator’s metadata (from a coordinator backup) to identify which shards were on the failed worker:
     ```sql
     -- On coordinator (from a restored coordinator backup)
     SELECT * FROM pg_dist_shard WHERE nodename = 'failed-worker-host';
     ```
   - Cross-reference with the restored worker’s data to extract the relevant shard data.

4. **Auditing Use Case**:
   - Example: You need to audit transactions from `my_table` on the failed worker as of April 1, 2025.
     - Restore the worker’s backup to April 1, 2025, using PITR.
     - Query the shard tables (e.g., `my_table_12345`) to extract transaction data.
     - Export the results to a CSV file for audit reporting:
       ```sql
       COPY (SELECT * FROM my_table_12345) TO '/var/tmp/audit_data.csv' WITH CSV HEADER;
       ```

5. **Coordinator Metadata for Context**:
   - If auditing requires cluster-wide context (e.g., shard placements or distributed table metadata), restore the coordinator’s backup to another instance and query `pg_dist_*` tables.
   - Example:
     ```sql
     SELECT logicalrelid, shardid, nodename, nodeport
     FROM pg_dist_shard
     JOIN pg_dist_placement ON pg_dist_shard.shardid = pg_dist_placement.shardid
     WHERE nodename = 'failed-worker-host';
     ```

#### Backup Retention for Auditing
- **Retention Policy**:
  - Retain backups for the duration required by your auditing or compliance needs (e.g., 7 years for financial audits).
  - For a 100TB cluster, use **incremental backups** with a retention policy (e.g., keep monthly full backups for 7 years, daily incrementals for 30 days) to manage storage costs.
- **Storage**:
  - Store audit backups in a secure, off-site location (e.g., AWS S3 with lifecycle policies for archival storage).
  - Use encryption for compliance (e.g., `pgBackRest` supports encryption).

#### If a Worker Fails
- **Operational Recovery**: Citus shard replication ensures the cluster remains operational by rerouting queries to surviving replicas. You can replace the failed worker (see rebalancing below), but this doesn’t address auditing needs.
- **Auditing Recovery**: Use the failed worker’s backup to restore its data to a separate instance for auditing, as described above. The coordinator’s metadata backup helps map shards to the restored data.

---

### 4. Does Citus Rebalancing Automatically Fill Data Back to a Recovered Worker?
**Yes, Citus shard rebalancing can restore data to a recovered or new worker**, but it’s **not fully automatic** in the sense of happening without intervention. Let’s clarify how rebalancing works and its role in recovering a failed worker.

#### How Citus Shard Rebalancing Works
- **Shard Rebalancing**:
  - Citus provides functions like `rebalance_table_shards()` to redistribute shards across worker nodes based on the current cluster configuration.
  - When a worker fails, Citus reroutes queries to surviving shard replicas (with `replication_factor = 2`). To restore the failed worker or add a new one, you rebalance shards to place replicas on the new/recovered worker.
- **Process**:
  - Add the recovered or new worker to the cluster:
    ```sql
    SELECT citus_add_node('new-worker-host', 5432);
    ```
  - Run rebalancing to move shards to the new worker:
    ```sql
    SELECT rebalance_table_shards('my_table');
    ```
  - Alternatively, use `citus_rebalance_start()` for background rebalancing (non-blocking):
    ```sql
    SELECT citus_rebalance_start();
    ```
- **Data Movement**:
  - Rebalancing copies shard data from surviving replicas to the new/recovered worker, ensuring each shard maintains the replication factor (e.g., 2).
  - This process is automatic once initiated, but it requires you to trigger the rebalance operation.

#### Automatic or Manual?
- **Not Fully Automatic**:
  - Citus does not automatically detect a recovered worker and start rebalancing. You must manually add the worker (`citus_add_node`) and initiate rebalancing (`rebalance_table_shards` or `citus_rebalance_start`).
  - However, Citus automates the actual data movement during rebalancing, copying shard data from surviving replicas to the new worker.
- **Citus Enterprise Features**:
  - Citus Enterprise and Citus Cloud offer **automated rebalancing** and **non-blocking rebalancing**, which can simplify recovery. Check if your Citus version supports these features (e.g., Citus 11.x or later).
  - Without these features, rebalancing is manual but straightforward.

#### Impact on a 100TB Cluster
- **Rebalancing Time**:
  - For a 100TB cluster, rebalancing is resource-intensive and time-consuming, as it involves copying large amounts of data across the network to the new worker.
  - Example: If a worker stores 10TB of shards (assuming 10 workers, 100TB total), rebalancing 10TB to a new worker could take hours or days, depending on network bandwidth, disk I/O, and cluster load.
- **Performance**:
  - Rebalancing can impact query performance, especially if done during peak usage. Use `citus_rebalance_start()` for non-blocking rebalancing or schedule during low-traffic periods.
- **Storage**:
  - Ensure the new/recovered worker has sufficient disk space (e.g., 10TB+ for its share of shards).

#### Worker Failure and Recovery
- **If a Worker Fails**:
  - Citus reroutes queries to surviving replicas, maintaining availability.
  - Add a new worker or recover the failed one, then rebalance to restore the replication factor:
    ```sql
    SELECT citus_add_node('new-worker-host', 5432);
    SELECT rebalance_table_shards();
    ```
- **Data Restoration**:
  - Rebalancing copies shard data from surviving replicas, so no backup is needed for operational recovery (assuming no corruption or multiple failures).
  - The coordinator’s metadata (`pg_dist_placement`) ensures the correct shards are placed on the new worker.

#### Backup vs. Rebalancing for Recovery
- **Rebalancing**:
  - Restores operational data to a new/recovered worker using surviving replicas.
  - Suitable for single worker failures but slow for large clusters (100TB).
  - Does not help with auditing, as it only restores the current state.
- **Backup**:
  - Restores historical data for auditing or catastrophic failures (e.g., multiple workers or coordinator loss).
  - Faster for recovering a failed worker than rebalancing, if the backup is recent:
    - Restore the worker’s backup to its data directory:
      ```bash
      pgbackrest --stanza=worker1 restore --db-path=/var/tellme/pgsql/data
      ```
    - Start the worker and let Citus synchronize metadata:
      ```sql
      SELECT citus_finish_pg_upgrade(); -- If post-upgrade
      ```
  - Essential for auditing, as described above.

#### Auditing and Failed Worker
- If a worker fails and you need to audit its data, rebalancing won’t help, as it only restores the current state to a new worker. Instead, use the worker’s backup to restore its data to a separate instance for auditing, as outlined in section 3.

---

### Recommended Backup Strategy for a 100TB Citus Cluster
Given the scale of your 100TB cluster and the auditing requirement, here’s a tailored backup strategy:

1. **Backup All Nodes**:
   - **Coordinator**:
     - Physical backup of `/var/tellme/pgsql/data` using `pgBackRest` or `pg_basebackup`.
     - Logical backup of metadata (`pg_dist_*` tables) using `COPY` or `pg_dump`.
     - Frequency: Daily incremental, weekly full, continuous WAL archiving.
   - **Workers**:
     - Physical backup of each worker’s `/var/tellme/pgsql/data` using `pgBackRest`.
     - Frequency: Staggered daily incremental (e.g., one worker per day), weekly full, continuous WAL archiving.
     - Optimize storage with compression and incremental backups.

2. **Auditing Retention**:
   - Retain backups for the required audit period (e.g., 7 years).
   - Use cloud storage (e.g., AWS S3 Glacier) with lifecycle policies to archive old backups cost-effectively.
   - Example `pgBackRest` retention:
     ```conf
     [global]
     repo1-retention-full=12  # Keep 12 full backups (e.g., 3 months if weekly)
     repo1-retention-archive=2555  # Keep 7 years of WAL for PITR
     ```

3. **Backup Storage**:
   - Store backups off-site to protect against data center failures.
   - For a 100TB cluster, expect compressed backups to be ~50–70TB (depending on data compressibility). Use incremental backups to reduce storage needs.

4. **Backup Tools**:
   - **pgBackRest**: Recommended for large clusters due to incremental backups, compression, encryption, and parallel processing.
     ```bash
     pgbackrest --stanza=coordinator backup
     pgbackrest --stanza=worker1 backup
     ```
   - **Barman**: Alternative for PostgreSQL backup management.
   - **Filesystem Snapshots**: If using a storage system with snapshot capabilities (e.g., ZFS, LVM), combine with `pgBackRest` for faster backups.

5. **Post-`pg_upgrade` Considerations**:
   - After upgrading from PostgreSQL 15 to 17 (as discussed), reconfigure backup jobs for the new PostgreSQL version (`/usr/pgsql-17/bin`).
   - Update `pgBackRest` or other tools to use PostgreSQL 17 binaries.
   - Verify Citus metadata backups post-upgrade:
     ```sql
     SELECT * FROM pg_dist_shard;
     ```

---

### Practical Steps for Worker Failure and Auditing
**Scenario**: A worker fails in your 100TB Citus cluster, and you need to recover operationally and audit its data.

1. **Operational Recovery**:
   - **Reroute Queries**: Citus automatically uses surviving shard replicas (`replication_factor = 2`), so the cluster remains operational.
   - **Replace Worker**:
     - Add a new worker:
       ```sql
       SELECT citus_add_node('new-worker-host', 5432);
       ```
     - Rebalance shards:
       ```sql
       SELECT citus_rebalance_start(); -- Non-blocking
       ```
     - Alternatively, restore the failed worker from a recent backup to speed up recovery:
       ```bash
       pgbackrest --stanza=worker1 restore --db-path=/var/tellme/pgsql/data
       /usr/pgsql-17/bin/pg_ctl -D /var/tellme/pgsql/data start
       ```
     - Update metadata if needed:
       ```sql
       SELECT citus_update_node('failed-worker-host', 'new-worker-host', 5432);
       ```

2. **Auditing**:
   - Restore the failed worker’s backup to a separate instance:
     ```bash
     pgbackrest --stanza=worker1 --target-time="2025-04-01 12:00:00" restore --db-path=/var/tellme/pgsql/audit_data
     /usr/pgsql-17/bin/pg_ctl -D /var/tellme/pgsql/audit_data start
     ```
   - Query shard tables for audit data:
     ```sql
     SELECT * FROM my_table_12345; -- Example shard table
     COPY (SELECT * FROM my_table_12345) TO '/var/tmp/audit_data.csv' WITH CSV HEADER;
     ```
   - Use coordinator metadata (from a coordinator backup) to identify relevant shards:
     ```sql
     SELECT * FROM pg_dist_shard WHERE nodename = 'failed-worker-host';
     ```

3. **Monitoring Rebalancing**:
   - Check rebalancing progress:
     ```sql
     SELECT * FROM citus_rebalance_status();
     ```
   - Verify shard placements post-rebalance:
     ```sql
     SELECT * FROM citus_shard_placements;
     ```

---

### Summary
- **Should You Run Backups?**: Yes, backups are essential for a 100TB Citus cluster with `replication_factor = 2` to protect against multiple failures, corruption, coordinator loss, and for auditing/compliance.
- **Back Up Each Worker?**: Yes, back up all workers to capture all shards, using incremental physical backups (e.g., `pgBackRest`) to optimize storage and time. Also back up the coordinator’s metadata.
- **Auditing with Backups**: Restore a failed worker’s backup to a separate instance to query historical shard data for auditing. Use coordinator metadata to map shards.
- **Rebalancing and Recovery**: Citus shard rebalancing restores data to a new/recovered worker by copying from surviving replicas, but it requires manual initiation (`citus_add_node`, `rebalance_table_shards`). It’s not automatic but effective for operational recovery, not auditing.
- **Recommended Strategy**:
  - Use `pgBackRest` for incremental backups of all nodes, with off-site storage and a 7-year retention policy for auditing.
  - Combine shard replication for HA with coordinator streaming replication for DR, as discussed previously.
  - For worker failures, use rebalancing for operational recovery and backups for auditing.

For a 100TB cluster, prioritize **incremental backups**, **off-site storage**, and **automation** (e.g., `pgBackRest` schedules, Citus rebalancing scripts) to manage complexity. If you need help setting up `pgBackRest`, automating rebalancing, or defining an audit workflow, let me know your Citus version, number of workers, or specific auditing requirements, and I can provide more detailed guidance!
