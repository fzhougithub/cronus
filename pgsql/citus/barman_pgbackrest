Given your context of managing a 100TB Citus cluster with PostgreSQL 17, a non-standard data directory (`/var/tellme/pgsql/data`), and an RPM-based system, and your interest in **pgBackRest** for incremental backups, you’ve now asked about **Barman** and whether it’s “the same thing” as pgBackRest. Below, I’ll provide a concise guide on how to **install and use Barman** for incremental PostgreSQL backups, tailored to your Citus cluster, and then **compare Barman and pgBackRest** to clarify their similarities, differences, and suitability for your setup. This will address your question directly while building on the prior pgBackRest instructions for consistency.

---

### Installing and Using Barman for Incremental PostgreSQL Backups

**Barman** (Backup and Recovery Manager) is an open-source tool for managing PostgreSQL physical backups, supporting full, incremental, and differential backups, as well as point-in-time recovery (PITR). It’s particularly suited for centralized backup management of multiple PostgreSQL instances, making it a viable option for your Citus cluster with a coordinator and multiple workers.

#### 1. Install Barman
Barman is available for Linux systems, including RPM-based distributions like CentOS, RHEL, or Rocky Linux. It’s typically installed from the PostgreSQL YUM repository or the 2ndQuadrant/EnterpriseDB repository.

**Steps**:
- **Add the PostgreSQL YUM Repository** (if not already done for pgBackRest):
  ```bash
  sudo yum install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-8-x86_64/pgdg-redhat-repo-latest.noarch.rpm
  sudo dnf -qy module disable postgresql
  ```
- **Install Barman**:
  - Install the Barman package:
    ```bash
    sudo yum install -y barman
    ```
  - Verify the installation:
    ```bash
    barman --version
    ```
    Output should show the installed version (e.g., `3.13.3` as of April 2025).[](https://pgbarman.org/)
- **Install on Backup Server**:
  - Unlike pgBackRest, Barman is typically installed on a **dedicated backup server** that manages backups for all PostgreSQL nodes (coordinator and workers). You don’t need to install Barman on each PostgreSQL node, but the backup server must have SSH access to all nodes.
  - Ensure the backup server has sufficient storage for backups (e.g., ~50–70TB compressed for your 100TB cluster).

#### 2. Configure Barman
Barman requires configuration on the backup server and PostgreSQL nodes to enable WAL archiving and backup operations.

**Steps**:
- **Create Barman Configuration**:
  - The main configuration file is `/etc/barman.conf` or `/etc/barman.d/*.conf`. Create a configuration for your Citus cluster:
    ```bash
    sudo mkdir -p /etc/barman.d
    sudo touch /etc/barman.d/citus.conf
    sudo chmod 640 /etc/barman.d/citus.conf
    sudo chown barman:barman /etc/barman.d/citus.conf
    ```
  - Example configuration for the coordinator and workers:
    ```ini
    [barman]
    barman_user = barman
    configuration_files_directory = /etc/barman.d
    barman_home = /var/lib/barman
    log_file = /var/log/barman/barman.log
    compression = gzip

    [coordinator]
    description = Citus Coordinator
    conninfo = host=coordinator-host user=barman password=your_password dbname=postgres
    backup_method = rsync
    streaming_archiver = on
    slot_name = barman_coordinator
    reuse_backup = link
    retention_policy = RECOVERY WINDOW OF 30 DAYS

    [worker1]
    description = Citus Worker 1
    conninfo = host=worker1-host user=barman password=your_password dbname=postgres
    backup_method = rsync
    streaming_archiver = on
    slot_name = barman_worker1
    reuse_backup = link
    retention_policy = RECOVERY WINDOW OF 30 DAYS

    [worker2]
    description = Citus Worker 2
    conninfo = host=worker2-host user=barman password=your_password dbname=postgres
    backup_method = rsync
    streaming_archiver = on
    slot_name = barman_worker2
    reuse_backup = link
    retention_policy = RECOVERY WINDOW OF 30 DAYS
    ```
  - **Notes**:
    - `barman_home = /var/lib/barman`: Directory for backup storage (local or mounted, e.g., NFS or cloud storage).
    - `conninfo`: Connection string for each PostgreSQL node. Replace `coordinator-host`, `worker1-host`, etc., with actual hostnames/IPs.
    - `backup_method = rsync`: Uses rsync for efficient file transfer (recommended for incremental backups).
    - `reuse_backup = link`: Enables incremental backups by hard-linking unchanged files from the previous backup, saving space.[](https://severalnines.com/blog/current-state-open-source-backup-management-postgresql/)
    - `streaming_archiver = on`: Enables WAL streaming via replication slots for near-zero RPO.[](https://www.reddit.com/r/PostgreSQL/comments/m3fibc/does_anyone_have_any_idea_about_the_difference/)
    - `retention_policy`: Keeps backups for 30 days; adjust to 7 years (`RECOVERY WINDOW OF 2555 DAYS`) for auditing.
    - Add a `[workerN]` section for each worker node.

- **Configure PostgreSQL Nodes**:
  - Create a `barman` user on each PostgreSQL node (coordinator and workers):
    ```sql
    CREATE ROLE barman WITH LOGIN PASSWORD 'your_password' SUPERUSER;
    ```
  - Edit `/var/tellme/pgsql/data/pg_hba.conf` on each node to allow the `barman` user to connect:
    ```conf
    host all barman backup-server-ip/32 md5
    host replication barman backup-server-ip/32 md5
    ```
  - Configure WAL archiving and replication in `/var/tellme/pgsql/data/postgresql.conf`:
    ```conf
    wal_level = replica
    archive_mode = on
    archive_command = 'barman-wal-archive coordinator-host coordinator %p'
    max_wal_senders = 10
    max_replication_slots = 10
    wal_keep_size = 1024  # 1GB of WAL
    ```
    - Replace `coordinator-host` and `coordinator` with the appropriate hostname and Barman server ID (`coordinator`, `worker1`, etc.) for each node.
  - Create a replication slot for each node:
    ```sql
    SELECT pg_create_physical_replication_slot('barman_coordinator'); -- On coordinator
    SELECT pg_create_physical_replication_slot('barman_worker1');     -- On worker1
    ```
  - Restart PostgreSQL on each node:
    ```bash
    sudo systemctl restart postgresql
    ```

- **Set Up SSH Access**:
  - Barman uses SSH for rsync-based backups. Generate SSH keys for the `barman` user on the backup server:
    ```bash
    sudo -u barman ssh-keygen -t rsa -b 4096 -N '' -f /var/lib/barman/.ssh/id_rsa
    ```
  - Copy the public key to each PostgreSQL node’s `postgres` user:
    ```bash
    sudo -u barman ssh-copy-id postgres@coordinator-host
    sudo -u barman ssh-copy-id postgres@worker1-host
    ```
  - Ensure the `postgres` user on each node trusts the `barman` user:
    ```bash
    sudo -u postgres mkdir -p /var/lib/pgsql/.ssh
    sudo -u postgres touch /var/lib/pgsql/.ssh/authorized_keys
    sudo -u postgres chmod 600 /var/lib/pgsql/.ssh/authorized_keys
    ```
  - Test SSH connectivity:
    ```bash
    sudo -u barman ssh postgres@coordinator-host
    ```

- **Initialize Barman**:
  - Check the configuration:
    ```bash
    sudo -u barman barman check coordinator
    sudo -u barman barman check worker1
    ```
  - If errors occur, fix connectivity, permissions, or PostgreSQL settings.

#### 3. Perform Initial Full Backup
Barman requires a full backup as the base for incremental backups.

**Steps**:
- **Coordinator Backup**:
  ```bash
  sudo -u barman barman backup coordinator
  ```
- **Worker Backups**:
  - For each worker:
    ```bash
    sudo -u barman barman backup worker1
    sudo -u barman barman backup worker2
    ```
  - Stagger backups for your 100TB cluster (e.g., one worker per day) to avoid overloading:
    ```bash
    sudo -u barman barman backup worker1  # Monday
    sudo -u barman barman backup worker2  # Tuesday
    ```

- **Notes**:
  - Barman uses `rsync` to copy the data directory (`/var/tellme/pgsql/data`), creating a full backup in `/var/lib/barman/coordinator/base/`.
  - For a 100TB cluster, full backups are large (~50–70TB compressed with `compression = gzip`). Ensure sufficient storage.

#### 4. Configure Incremental Backups
Barman supports **incremental backups** by reusing unchanged files from the previous backup (`reuse_backup = link`), reducing backup time and storage.

**Steps**:
- **Enable Incremental Backups**:
  - Ensure `reuse_backup = link` is set in `/etc/barman.d/citus.conf` for each stanza. This hard-links unchanged files, saving space.[](https://severalnines.com/blog/current-state-open-source-backup-management-postgresql/)
  - Barman automatically treats subsequent backups as incremental if `reuse_backup` is enabled.

- **Schedule Incremental Backups**:
  - Run incremental backups daily:
    ```bash
    sudo -u barman barman backup coordinator
    sudo -u barman barman backup worker1
    ```
  - Unlike pgBackRest, Barman doesn’t explicitly distinguish between full and incremental backups in the command; `reuse_backup = link` makes backups incremental by default.
  - Example schedule:
    - Weekly full backup (e.g., Sunday, by clearing old backups or forcing a full backup):
      ```bash
      sudo -u barman barman backup coordinator --reuse-backup=off
      ```
    - Daily incremental backup (Monday–Saturday):
      ```bash
      sudo -u barman barman backup coordinator
      ```

- **Automate with Cron**:
  - Create a cron job for the `barman` user:
    ```bash
    sudo crontab -u barman -e
    ```
    Add:
    ```cron
    # Daily incremental backup at 2 AM
    0 2 * * * /usr/bin/barman backup coordinator
    # Weekly full backup on Sunday at 3 AM
    0 3 * * 0 /usr/bin/barman backup coordinator --reuse-backup=off
    # Worker1 incremental at 2:30 AM
    30 2 * * * /usr/bin/barman backup worker1
    # Worker1 full on Monday at 3 AM
    0 3 * * 1 /usr/bin/barman backup worker1 --reuse-backup=off
    ```
  - Stagger worker backups to avoid overlap.

- **WAL Archiving**:
  - Barman uses WAL streaming (`streaming_archiver = on`) via replication slots for near-zero RPO.[](https://www.reddit.com/r/PostgreSQL/comments/m3fibc/does_anyone_have_any_idea_about_the_difference/)
  - Verify WAL archiving:
    ```bash
    sudo -u barman barman receive-wal coordinator
    sudo -u barman barman check coordinator
    ```

#### 5. Verify Backups
- **List Backups**:
  ```bash
  sudo -u barman barman list-backup coordinator
  sudo -u barman barman list-backup worker1
  ```
  Output shows backup IDs, timestamps, and sizes (e.g., `20250509T030000`).
- **Check Backup Integrity**:
  ```bash
  sudo -u barman barman check-backup coordinator 20250509T030000
  ```

#### 6. Restore from a Backup (For Recovery or Auditing)
To recover a failed worker or audit historical data, restore a backup to a new data directory.

**Steps**:
- **Stop PostgreSQL** (if restoring to the original node):
  ```bash
  sudo systemctl stop postgresql
  ```
- **Restore a Backup**:
  - Example: Restore the latest backup for worker1:
    ```bash
    sudo -u barman barman recover worker1 latest /var/tellme/pgsql/data
    ```
  - For **auditing** (e.g., data as of April 1, 2025), restore with PITR:
    ```bash
    sudo -u barman barman recover worker1 latest /var/tellme/pgsql/audit_data --target-time "2025-04-01 12:00:00"
    ```
  - Note: PITR requires archived WAL files.

- **Start PostgreSQL**:
  - For operational recovery:
    ```bash
    /usr/pgsql-17/bin/pg_ctl -D /var/tellme/pgsql/data start
    ```
  - For auditing:
    ```bash
    /usr/pgsql-17/bin/pg_ctl -D /var/tellme/pgsql/audit_data start
    ```
  - Query shard tables for auditing (e.g., `my_table_12345`).

- **Update Citus Metadata** (if recovering a worker):
  ```sql
  SELECT citus_update_node('failed-worker-host', 'restored-worker-host', 5432);
  SELECT citus_finish_pg_upgrade();  # If post-upgrade
  ```

#### 7. Additional Configuration for a 100TB Citus Cluster
- **Centralized Management**:
  - Barman runs on a single backup server, simplifying management for multiple nodes.[](https://dev.to/hujan/barman-vs-pgbackrest-2bi6)
  - Store backups in a large filesystem or cloud storage (e.g., AWS S3 via a mounted filesystem).
- **Compression**:
  - Use `compression = gzip` or `pigz` for faster compression:
    ```ini
    compression = pigz
    ```
- **Retention Policy**:
  - Set `retention_policy = RECOVERY WINDOW OF 2555 DAYS` for 7-year auditing:
    ```ini
    retention_policy = RECOVERY WINDOW OF 2555 DAYS
    ```
- **Staggered Backups**:
  - Schedule backups for 10 workers (e.g., worker1 on Monday, worker2 on Tuesday) to distribute load.
- **WAL Streaming**:
  - Barman’s WAL streaming reduces RPO compared to pgBackRest’s asynchronous archiving.[](https://www.reddit.com/r/PostgreSQL/comments/m3fibc/does_anyone_have_any_idea_about_the_difference/)
- **Post-`pg_upgrade`**:
  - Update Barman to use PostgreSQL 17 binaries and reconfigure WAL streaming post-upgrade.

---

### Comparing Barman and pgBackRest

To address whether Barman is “the same thing” as pgBackRest, here’s a detailed comparison based on your 100TB Citus cluster’s needs, focusing on incremental backups, ease of use, and suitability for disaster recovery (DR) and auditing. I’ll incorporate insights from web sources where relevant.[](https://severalnines.com/blog/current-state-open-source-backup-management-postgresql/)[](https://www.reddit.com/r/PostgreSQL/comments/m3fibc/does_anyone_have_any_idea_about_the_difference/)[](https://dev.to/hujan/barman-vs-pgbackrest-2bi6)

#### Similarities
- **Backup Types**:
  - Both support **full, incremental, and differential backups** and **PITR** for physical backups of PostgreSQL data directories.
  - Incremental backups copy only changed files since the last backup, reducing storage and time for your 100TB cluster.
- **WAL Archiving**:
  - Both integrate with PostgreSQL’s WAL archiving for PITR, essential for auditing (e.g., restoring to April 1, 2025).
- **Compression**:
  - Both offer compression (e.g., gzip, Zstandard) to reduce backup size (~50–70TB compressed for 100TB).
- **Remote Storage**:
  - Both support off-site storage (pgBackRest via S3, Barman via mounted filesystems or external tools).
- **Citus Compatibility**:
  - Both work with Citus by backing up the coordinator’s metadata and workers’ shards. Coordinator metadata (`pg_dist_*` tables) can be exported separately for both.
- **Open Source**:
  - Both are open-source, with active communities and enterprise support (e.g., Crunchy Data for pgBackRest, EnterpriseDB for Barman).[](https://pgbackrest.org/)

#### Differences
| **Feature**                     | **pgBackRest**                                                                 | **Barman**                                                                      |
|---------------------------------|-------------------------------------------------------------------------------|--------------------------------------------------------------------------------|
| **Architecture**                | Distributed: Runs on each PostgreSQL node or a backup server.                  | Centralized: Runs on a dedicated backup server managing multiple nodes. |[](https://dev.to/hujan/barman-vs-pgbackrest-2bi6)
| **Incremental Backup**          | Explicit `--type=incr` for incremental backups; copies changed files since last backup. | Implicit with `reuse_backup = link`; hard-links unchanged files, treating backups as incremental. |[](https://severalnines.com/blog/current-state-open-source-backup-management-postgresql/)
| **Differential Backup**         | Copies changes since last full backup (`--type=diff`).                        | Uses WALs for “differential” backups (last backup + changes), less explicit. |[](https://severalnines.com/blog/current-state-open-source-backup-management-postgresql/)
| **WAL Archiving**               | Asynchronous (`archive-push`), small RPO risk (seconds of data loss).         | Streaming via replication slots, near-zero RPO.                        |[](https://www.reddit.com/r/PostgreSQL/comments/m3fibc/does_anyone_have_any_idea_about_the_difference/)
| **Performance**                 | Parallel backups/restores with `process-max` (e.g., 8 cores), faster for large DBs. | Rsync-based, single-threaded per backup, slower for 100TB.            |[](https://pgbackrest.org/)[](https://www.reddit.com/r/PostgreSQL/comments/og5p07/300gb_postgres_backup_to_local_diskobject_storage/)
| **Storage Efficiency**          | Zstandard (`zst`) for high compression; efficient for large clusters.         | Gzip or pigz; `reuse_backup = link` saves space via hard-links.        |[](https://severalnines.com/blog/current-state-open-source-backup-management-postgresql/)
| **Remote Restore**              | Requires SSH or manual transfer for remote restores.                  | Native remote restore to another server.                              |[](https://www.postgresql.org/message-id/78135d4e-02c1-f757-fe26-3d92d25623f5%40pgmasters.net)[](https://www.postgresql.org/message-id/78135d4e-02c1-f757-fe26-3d92d25623f5%40pgmasters.net)
| **Backup from Standby**         | Not supported natively.                                                      | Supports backups from standby nodes, reducing primary I/O load.       |[](https://severalnines.com/blog/current-state-open-source-backup-management-postgresql/)
| **Ease of Setup**               | Simpler for single nodes, more config for multi-node clusters.                | Easier for centralized management, complex SSH setup for multi-node. |[](https://opensource-db.com/unveiling-the-mystery-of-postgres-backups-for-my-peeps/)
| **Retention Policy**            | Explicit full/diff retention (`repo1-retention-full`).                        | Flexible with `RECOVERY WINDOW`, auto-converts incremental to full.   |[](https://www.postgresql.org/message-id/5aef9b65-d7a9-6a77-149d-8693071c1eae%40matrix.gatewaynet.com)
| **Scalability**                 | Scales well for large DBs (100TB+) with parallel processing.        | Better for small/medium DBs or multi-server management.               |[](https://pgbackrest.org/)[](https://dev.to/hujan/barman-vs-pgbackrest-2bi6)
| **S3 Support**                  | Native S3 integration for off-site DR.                              | Requires external tools or mounted S3 filesystem.                              |[](https://pgbackrest.org/)
| **Backup Resume**               | Resumes interrupted backups, reducing load on PostgreSQL.           | No native resume; restarts failed backups.                                    |[](https://pgbackrest.org/)
| **Database Exclusion**          | Can exclude databases during restore (zeroed out).                   | Restores entire cluster, no database-level exclusion.                         |[](https://www.reddit.com/r/PostgreSQL/comments/og5p07/300gb_postgres_backup_to_local_diskobject_storage/)

#### Key Considerations for Your 100TB Citus Cluster
- **Incremental Backups**:
  - **pgBackRest**: Explicitly supports incremental backups (`--type=incr`), optimized for large datasets with parallel processing. For 10 workers (~10TB each), pgBackRest’s parallelism (`process-max=8`) speeds up backups significantly.
  - **Barman**: Incremental backups via `reuse_backup = link` are space-efficient due to hard-linking, but rsync is single-threaded, making backups slower for 100TB.[](https://severalnines.com/blog/current-state-open-source-backup-management-postgresql/)
  - **Winner**: pgBackRest, for faster incremental backups in a large cluster.

- **Performance**:
  - **pgBackRest**: Parallel processing and efficient compression (Zstandard) make it ideal for your 100TB cluster, reducing backup and restore times.[](https://pgbackrest.org/)
  - **Barman**: Rsync-based backups are slower, especially for large data directories, and lack native parallelism.[](https://www.reddit.com/r/PostgreSQL/comments/og5p07/300gb_postgres_backup_to_local_diskobject_storage/)
  - **Winner**: pgBackRest, for better performance at scale.

- **WAL Archiving and RPO**:
  - **pgBackRest**: Asynchronous WAL archiving risks small data loss (seconds), suitable if you use Citus shard replication (`replication_factor = 2`) or coordinator streaming replication for HA.[](https://www.reddit.com/r/PostgreSQL/comments/m3fibc/does_anyone_have_any_idea_about_the_difference/)
  - **Barman**: WAL streaming via replication slots offers near-zero RPO, ideal for critical data where no loss is tolerable.[](https://www.reddit.com/r/PostgreSQL/comments/m3fibc/does_anyone_have_any_idea_about_the_difference/)
  - **Winner**: Barman, if zero RPO is critical; pgBackRest otherwise.

- **Centralized Management**:
  - **pgBackRest**: Requires configuration on each node or a backup server, less centralized.
  - **Barman**: Runs on a single backup server, simplifying management for your 10 workers and coordinator.[](https://dev.to/hujan/barman-vs-pgbackrest-2bi6)
  - **Winner**: Barman, for easier multi-node management.

- **Storage and DR**:
  - **pgBackRest**: Native S3 support is perfect for off-site DR, critical for your 100TB cluster’s disaster recovery.[](https://pgbackrest.org/)
  - **Barman**: Requires external tools for S3, less seamless for cloud storage.
  - **Winner**: pgBackRest, for integrated off-site storage.

- **Auditing**:
  - Both support PITR for auditing (e.g., restoring a worker to April 1, 2025). Barman’s `RECOVERY WINDOW` simplifies long-term retention (7 years), while pgBackRest requires explicit retention settings.[](https://www.postgresql.org/message-id/5aef9b65-d7a9-6a77-149d-8693071c1eae%40matrix.gatewaynet.com)
  - **Winner**: Barman, for flexible retention; pgBackRest is comparable with proper configuration.

- **Ease of Use**:
  - **pgBackRest**: Simpler for single-node setups but requires more configuration for a multi-node Citus cluster.
  - **Barman**: Centralized management is user-friendly, but SSH setup and rsync configuration can be complex.
  - **Winner**: Barman, for centralized ease; pgBackRest is straightforward with automation.

- **Citus Integration**:
  - Both handle Citus well by backing up the coordinator’s metadata and workers’ shards. Barman’s standby backup support could offload I/O from primary workers, but this is less critical with `replication_factor = 2`.[](https://severalnines.com/blog/current-state-open-source-backup-management-postgresql/)
  - **Winner**: Tie, both are compatible.

#### Are They the Same Thing?
**No, Barman and pgBackRest are not the same**, though they share similar goals (physical backups, incremental backups, PITR). Key differences include:
- **Architecture**: Barman is centralized (backup server), pgBackRest is distributed (node or server).[](https://dev.to/hujan/barman-vs-pgbackrest-2bi6)
- **Performance**: pgBackRest is faster for large clusters due to parallelism.[](https://pgbackrest.org/)
- **WAL Handling**: Barman’s WAL streaming offers better RPO.[](https://www.reddit.com/r/PostgreSQL/comments/m3fibc/does_anyone_have_any_idea_about_the_difference/)
- **Storage**: pgBackRest’s native S3 support is more seamless for DR.[](https://pgbackrest.org/)
- **Management**: Barman simplifies multi-node management, pgBackRest requires more node-level config.[](https://dev.to/hujan/barman-vs-pgbackrest-2bi6)

#### Recommendation for Your 100TB Citus Cluster
- **Choose pgBackRest** for your setup, as it’s better suited for a 100TB Citus cluster due to:
  - **Performance**: Parallel backups/restores are critical for 10 workers (~10TB each), reducing backup windows.[](https://pgbackrest.org/)
  - **S3 Integration**: Native S3 support ensures off-site DR, essential for your scale.[](https://pgbackrest.org/)
  - **Scalability**: Optimized for large databases, with features like backup resume and database exclusion.[](https://www.reddit.com/r/PostgreSQL/comments/og5p07/300gb_postgres_backup_to_local_diskobject_storage/)[](https://pgbackrest.org/)
  - **Citus Fit**: Handles coordinator and worker backups efficiently, integrating with your `pg_upgrade` workflow (PostgreSQL 17, `/var/tellme/pgsql/data`).
- **Use Barman** if:
  - You prioritize **centralized management** and have a dedicated backup server.
  - **Zero RPO** is critical, and you’re willing to trade performance for WAL streaming.[](https://www.reddit.com/r/PostgreSQL/comments/m3fibc/does_anyone_have_any_idea_about_the_difference/)
  - You prefer **flexible retention policies** for auditing (7 years).[](https://www.postgresql.org/message-id/5aef9b65-d7a9-6a77-149d-8693071c1eae%40matrix.gatewaynet.com)
- **Hybrid Approach**:
  - Use **pgBackRest for workers** due to performance and S3 support, backing up each worker’s shards.
  - Consider **Barman for the coordinator** if zero RPO for metadata is critical, using a centralized backup server with WAL streaming.

#### Practical Workflow with Barman (if chosen)
- **Setup**:
  - Install Barman on a backup server, configure SSH, and set up stanzas for coordinator and 10 workers.
  - Enable `reuse_backup = link` for incremental backups.
- **Backup Schedule**:
  - Weekly full backups (e.g., `barman backup coordinator --reuse-backup=off`).
  - Daily incremental backups (e.g., `barman backup worker1`).
  - Stagger worker backups (e.g., worker1 on Monday, worker2 on Tuesday).
- **Auditing**:
  - Restore a worker’s backup for auditing:
    ```bash
    barman recover worker1 latest /var/tellme/pgsql/audit_data --target-time "2025-04-01 12:00:00"
    ```
  - Use coordinator metadata backups (`pg_dist_*` tables) for shard mapping.
- **Recovery**:
  - Restore failed workers from backups, update Citus metadata, or use rebalancing for operational recovery (as discussed previously).

#### Post-`pg_upgrade` Notes
- Both tools require updating to PostgreSQL 17 binaries post-upgrade:
  - **pgBackRest**: Set `pg1-bin=/usr/pgsql-17/bin` in `/etc/pgbackrest.conf`.
  - **Barman**: Update `conninfo` and WAL streaming settings for PostgreSQL 17.
- Restore SELinux contexts after restores:
  ```bash
  sudo restorecon -r /var/tellme/pgsql/data
  ```

---

### Summary
- **Barman Installation and Use**:
  - Install on a backup server (`yum install barman`).
  - Configure centralized backups with `reuse_backup = link` for incremental backups.
  - Schedule full (weekly) and incremental (daily) backups via cron, using rsync and WAL streaming.
  - Restore for recovery or auditing with PITR.
- **pgBackRest vs. Barman**:
  - **Similarities**: Both support incremental backups, PITR, and Citus, with compression and WAL archiving.
  - **Differences**: pgBackRest excels in performance (parallelism) and S3 support; Barman offers centralized management and near-zero RPO via WAL streaming.
  - **Recommendation**: Use **pgBackRest** for your 100TB Citus cluster due to its scalability, performance, and DR capabilities. Consider **Barman** for the coordinator if zero RPO or centralized management is a priority.
- **Your Setup**: pgBackRest aligns better with your large-scale, distributed cluster and prior `pg_upgrade` workflow, but Barman is a strong alternative for centralized management.

If you need help setting up Barman for specific workers, integrating with AWS S3 for Barman, or automating either tool for your 10-worker cluster, let me know your OS details, Citus version, or specific requirements, and I can provide further tailored guidance!
