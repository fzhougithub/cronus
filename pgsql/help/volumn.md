1, Terraform change will reflecte to
vgdisplay, lvdisplay, pvdisplay
lsblk or blkid
/etc/lvm/lvm.conf
man lvm.conf
man lvmconfig
/etc/lvm
LVM_SYSTEM_DIR
/etc/lvm/backup/
/etc/lvm/archive/
/etc/lvm/cache/.cache
/var/lock/lvm/
/sys/block/
/proc/partitions
/etc/fstab
/etc/lvm/archive/<VG>
/etc/lvm/backup/<VG>

The direct command should be
lvextend -L +2G /dev/logsvg/pg_backups
or
lvextend -l +100%FREE /dev/logsvg/pg_backups

then, resize filesystem
resize2fs /dev/logsvg/pg_backups
or 
xfs_growfs /mount/point

then, verify the change
lvdisplay /dev/logsvg/pg_backups

Add new disk to the volumn
pvcreate /dev/sdb
vgextend logsvg /dev/sdb

Physical Volumes (PVs): These are actual block devices (e.g., /dev/sda1, /dev/sdb) initialized for LVM use.
Volume Groups (VGs): These are pools of storage created by combining one or more PVs. Think of a VG as a flexible container of raw space.
Logical Volumes (LVs): These are slices of space carved out from a VG. They behave like partitions but are far more flexible.
PVs = Bricks
VG = Warehouse built from bricks
LVs = Rooms inside the warehouse

vgs # check the vg info
[root@ods-db-01-alma8-uatpls ~]$ vgs
  VG        #PV #LV #SN Attr   VSize     VFree
  appsvg      1   1   0 wz--n-    10.35t 100.00m
  backupsvg   1   1   0 wz--n-    <3.42t      0
  logsvg      1   1   0 wz--n- <1003.00g      0
  rootvg      1   6   0 wz--n-    41.50g  <5.86g
  swapsvg     1   1   0 wz--n-   <96.00g      0

vgs and vgdisplay show that!

lsblk -o NAME,VG,LV,MOUNTPOINT

lvs --segments | grep /apps

lvextend -L +2T /dev/appsvg/appslv
xfs_growfs /apps

mapper node default concatenation
/dev/mapper/appsvg-appslv

lvdisplay /dev/mapper/appsvg-appslv

 LV Path                /dev/appsvg/appslv
  LV Name                appslv
  VG Name                appsvg
  LV UUID                JXsuZ2-VUU4-SKK6-3WGC-QemO-Z9O6-RxuMiE
  LV Write Access        read/write
  LV Creation host, time 10-222-200-71.ssnc-corp.cloud, 2024-07-01 21:34:30 +0000
  LV Status              available
  # open                 1
  LV Size                10.35 TiB
  Current LE             2713830
  Segments               1
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     8192
  Block device           253:7

[root@ods-db-01-alma8-uatpls ~]$ vgs appsvg
  VG     #PV #LV #SN Attr   VSize  VFree
  appsvg   1   1   0 wz--n- 10.35t 100.00m

lvs --noheadings -o vg_name /dev/mapper/appsvg-appslv

lvs vgs and pvs
haha

to check the metadata saved in disk, use
vgcfgbackup appsvg
then check
/etc/lvm/archive/appsvg_*.vg


[root@ods-db-01-alma8-uatpls ~]$ diff /etc/lvm/archive/appsvg_00030-1769143418.vg /etc/lvm/archive/appsvg_00029-257904899.vg
1c1
< # Generated by LVM2 version 2.03.14(2)-RHEL8 (2021-10-20): Mon Aug 11 16:08:09 2025
---
> # Generated by LVM2 version 2.03.14(2)-RHEL8 (2021-10-20): Mon Aug 11 16:07:56 2025
6c6
< description = "Created *before* executing 'lvresize -r -L +1000G /dev/mapper/appsvg-appslv'"
---
> description = "Created *before* executing 'pvresize /dev/sdd'"
9c9
< creation_time = 1754928489	# Mon Aug 11 16:08:09 2025
---
> creation_time = 1754928476	# Mon Aug 11 16:07:56 2025
13c13
< 	seqno = 31
---
> 	seqno = 30
30c30
< 			dev_size = 22231906304	# 10.3525 Terabytes
---
> 			dev_size = 20134754304	# 9.37598 Terabytes
32c32
< 			pe_count = 2713855	# 10.3525 Terabytes
---
> 			pe_count = 2457855	# 9.37597 Terabytes

haha. 
so, the pvresize, run first, means the bricks incresed
then, lvresize, increase the lv. 

10.35 TB = 10,350 GiB = 10,598,400 MiB
PE size = 4 MiB
pe_count = 10,598,400 / 4 = 2,649,600

You can use lvextend -l +1000 to add 1000 extents, or -L +4G to add by size.
PE size affects how LVM allocates space and handles snapshots, mirrors, and migrations.
Block size affects how efficiently files are stored and how fast theyâ€™re read/written.


Feature	Physical Extent (PE)	Filesystem Block Size
Layer	LVM	Filesystem
Typical Size	4 MiB	4 KiB
Purpose	LV allocation	File data I/O
Configured via	vgcreate	mkfs or tune2fs
Affects	LV sizing, snapshots	Disk usage, performance

VG size = sum of all PV sizes.


/etc/lvm/lvm.conf
/etc/lvm/archive/	
/etc/lvm/backup/	

vgcfgbackup appsvg
cat /etc/lvm/backup/appsvg

VG always equal to the PVs sum. 
# Create two PVs
pvcreate /dev/sdb1  # 100 GiB
pvcreate /dev/sdc1  # 50 GiB
# Create VG from both
vgcreate myvg /dev/sdb1 /dev/sdc1
# VG size will be ~150 GiB
vgs

/usr/bin/rescan-scsi-bus.sh
The specific sections of the code that facilitate this process are:
findresized() function: This function checks if the size of a block device has changed at the hardware level. It does this by reading the current size from sysfs and then writing a 1 to the rescan file for that device. It then re-reads the size and compares it to the original. If they are different, the script reports the device as resized.
resizempaths() function: If you're using multipath, this function uses the multipathd command to resize the multipath device map. This step is necessary to tell the multipath daemon about the newly available space on the underlying paths.
In your increase disk size operation, the procedure would look like this:
Increase Storage on SAN/Hypervisor: You or the storage administrator expands the LUN or virtual disk.
Run rescan-scsi-bus.sh: You run this script with the appropriate flags (e.g., -s or --resize) to make the kernel aware of the new size.
Resize LVM: You run LVM commands like pvresize to extend the Physical Volume, followed by lvresize to extend the Logical Volume.
Resize Filesystem: You run a filesystem-specific command like xfs_growfs or resize2fs to finally extend the filesystem to fill the newly available space.


